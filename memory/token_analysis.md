# Token 限制技术分析

## 各模型上下文窗口大小

| 模型 | 上下文窗口 | 说明 |
|------|-----------|------|
| DeepSeek Chat | 64K tokens | 约 65,536 tokens |
| DeepSeek V3.2 | 128K tokens | 约 131,072 tokens |
| GPT-3.5-turbo | 16K tokens | 约 16,384 tokens |
| GPT-4 | 8K/32K tokens | 取决于版本 |
| Claude 3 Sonnet | 200K tokens | 约 200,000 tokens |

## Token 计算规则

### 中文文本
- **1 个中文字符 ≈ 1-2 tokens**（平均约 1.5 tokens）
- **1 个英文单词 ≈ 1-2 tokens**

### 消息格式开销
每条消息包含：
- `{"role": "user", "content": "..."}` 格式开销：约 10-15 tokens
- 系统消息通常较长（人设信息）：可能 500-2000 tokens

## 实际容量计算

### 假设条件
- 系统消息（人设）：1500 tokens
- 平均每条用户消息：50 tokens（约 30-40 字）
- 平均每条 AI 回复：100 tokens（约 60-80 字）
- 每条消息格式开销：15 tokens

### DeepSeek Chat (64K tokens)
```
可用 tokens = 64,000 - 1,500 (系统消息) = 62,500 tokens
每条对话轮次 = 50 + 15 (用户) + 100 + 15 (AI) = 180 tokens
可存储轮次 = 62,500 / 180 ≈ 347 轮对话
```

### DeepSeek V3.2 (128K tokens)
```
可用 tokens = 128,000 - 1,500 = 126,500 tokens
可存储轮次 = 126,500 / 180 ≈ 702 轮对话
```

### GPT-3.5-turbo (16K tokens)
```
可用 tokens = 16,000 - 1,500 = 14,500 tokens
可存储轮次 = 14,500 / 180 ≈ 80 轮对话
```

## 当前实现的问题

**按消息数量限制（max_length=20）的问题：**
- 如果消息很短：可能只用了 5,000 tokens，浪费了大量空间
- 如果消息很长：可能 20 条消息就超过 64K tokens，导致截断
- **不够精确**：无法充分利用模型的上下文窗口

## 建议的改进方案

### 方案1：基于 Token 数量管理（推荐）
- 统计每条消息的 token 数量
- 当总 token 数接近限制时，删除最旧的消息
- 更精确地利用上下文窗口

### 方案2：混合策略
- 设置最小消息数量（如 10 条）
- 设置最大 token 数量（如 50K tokens）
- 同时满足两个条件才删除旧消息

